<table>
	<tr>
		<th class="pros">Pros</th>
		<th class="cons">Cons</th>
	</tr>
	<tr>
		<td class="pros">Easily interpretable, insensitive to outliers, self-selects relevant features.</td>
		<td class="cons">Easy to overfit, inefficient.</td>
	</tr>
</table>

<p>CART classification is the same algorithm as CART regression except that new values take on the category of the leaf node instead of the mean of the leaf node. The impurity function used is different.</p>

<h2>CART</h2>

<p>Classification and regression tree (CART) is the umbrella term from learning using decision trees. The algorithm makes binary splits within features based on some heuristic, originally the <span class="name">ID3</span> algorithm that calculates <span class="name">information gain</span> by comparing entropies. However, CART in modern libraries use a number of other functions to decide the best splits. After CART makes all the splits, a prediction of a categorical point is given the category of the partition, or <span class="name">leaf</span> it falls under, and a prediction of a real value takes the mean of all the points within the leaf.</p>

<p>There are several reasons to use decision trees for regression:
	<ul>
		<li>They select the most relevant features automatically. The algorithm searches through all the points to make the best split according to the hueristic.</li>
		<li>They are insensitive to missing values, scaling, or outliers, since they deal in proportions and not absolute values. Therefore, they reduce preparation time.</li>
		<li>Linearity is not assumed. Splits are possible regardless of the shape of the dataset. This becomes valuable under high dimensionality.</li>
		<li>They are easy to interpret. Splits on features can be easily visualized.</li>
	</ul>
</p>

<p>However, trees have several major problems as well:
	<ul>
		<li>They are unstable. Small changes or additions to the predictor variables can cause large changes in tree structure.</li>
		<li>Preparing decision trees is a costly procedure that involves going through each feature of each example, calculating their value according to the heuristic, and comparing them, on each iteration.</li>
		<li>They are prone to overfitting. There is no easy way to determine the optimal number of splits while building the tree. It may be done with cross-validation, which adds to the time complexity cost.</li>
	</ul>
</p>

<p>The CART algorithm performs an exhaustive search between each data point in node \(m = \mathcal{D} | (\theta^{*}_1, \dots, \theta^{*}_{k-1})\) (the subset of data given previous splits) on each feature for candidate splits \(\theta(j, t_m)\), where \(j\) is the feature and \(t_m\) is the threshold. For each candidate split, it partitions the data into \(P_{left} (\theta)\) and \(P_{right} (\theta)\) subsets. It only needs to search boundaries within categorical features where classes change, such that \(x^{(i)}_j \neq x^{(i+1)}_j)\, because any shift from those points will necessarily increase error. </p>

$$
\begin{align}
P_{left} (\theta) & = (x^{(i)}, y^{(i)}) | x^{(i)}_j \leq t_m, \hspace{5mm} i=1,\dots\,n \\
P_{right} (\theta) & = P_{all} \setminus P_{left} (\theta)
\end{align}
$$

<p>We want to find the split \(\theta^{*}\) that minimizes the impurity, based on the impurity function \(H()\), which is the heuristic used. Let \(n_{left}\) and \(n_{right}\) be the number of points in node \(m\) in the left and right partitions, respectively. Let \(N_m\) be the total number of points in node \(m\). We define the cost function</p>

$$G(P, \theta) = \frac{n_{left}}{N_m} H(P_{left} (\theta)) + \frac{n_{right}}{N_m} H(P_{right} (\theta))$$

<p>Therefore, our objective is</p>

$$\theta^{*} = \mathrm{min}_{\theta} G(P, \theta)$$

<p>These steps are recursed for subsets \(P_{left}(\theta^{*})\) and \(P_{left}(\theta^{*})\) until an end condition is reached, whether
<ul>
	<li>\(N_m = 1\). There is one point left in each node.</li>
	<li>\(N_m < \textrm{min}_{samples}\). Maximum allowable depth is reached.</li>
	<li>\(G(\theta_{k-1}) - G(\theta_k) < t\). Reduction in impurity is less than some predefined threshold.</li>
</ul>
</p>

<h2>Impurity</h2>

<p>The impurity function for decision tree classification compares a metric function on a set against the subset. <span class="name">Gini index</span>, a measure of the probability a randomly selected element is mislabeled, is almost universally used. Let \(Pr(y_i)\) be the probability of a single class. Then the sum of the probabilities of all other classes is the probability of mislabeling the class, \(\sum_{j \neq i} Pr(y_j) = 1 - Pr(y_i)\), \(i=1,\dots, k\). The Gini index sums up mislabeling of all classes. Let \(S\) be the set.</p>

$$
\begin{align}
H_{Gini}(P) & = \sum_{i=1}^k Pr(y_i) \sum_{j \neq i} Pr(y_j) \\
& = \sum_{i=1}^k Pr(y_i) (1 - Pr(y_i)) \\
& = \sum_{i=1}^k (Pr(y_i) - Pr(y_i)^2) \\
& = \sum_{i=1}^k Pr(y_i) - \sum_{i=1}^k Pr(y_i)^2 \\
& = 1 - \sum{i=1}^k Pr(y_i)^2
\end{align}
$$

<p>Historically, and for exploratory analysis, <span class="name">entropy</span> is also used. Entropy is the average amount of information produced. Suppose we want to encode the probability of a class, for example with Huffman coding. Then it stands to reason that the lower the probability, the more bits it would take to encode it. For example, flipping a coin with \(Pr(y) = \frac{1}{2}\) produces 1 bit of information, where the result is either 0 or 1. Therefore, the <span class="name">information content</span> function is</p>

$$
\begin{align}
I(y_i) & = \mathrm{log}_2 (\frac{1}{Pr(y_i)}) \\
& = - \mathrm{log}_2 (Pr(y_i))
\end{align}
$$

<p>Entropy, then, is the sum of probabilities of the information contents of each class.</p>

$$
\begin{align}
H_{entropy}(P) & = \sum_{i=1}^k Pr(y_i) I(y_i) \\
& = - \sum_{i=1}^k Pr(y_i) \mathrm{log}_2 (Pr(y_i))
\end{align}
$$

<p>Note that an alternative formulation is to maximize the gain function instead of minimizing the cost function. The gain function here is either the Gini gain or the information gain (for entropy).</p>

<p>The two metrics are, for all intents and purposes, indistinguishable. A mathematically exhaustive study proved that they differ in less than 2% of conditions (<a href="https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf">source</a>). However, Gini impurity is usually used since it is less computationally expensive to calculate a square than logarithms.</p>

<p>It is recommended to <span class="important">use cross-validation to find the optimal depth to prevent overfitting</span>.</p>