<table>
	<tr>
		<th class="pros">Pros</th>
		<th class="cons">Cons</th>
	</tr>
	<tr>
		<td class="pros">Bagging produces good performance, non-linear.</td>
		<td class="cons">Easy to overfit, very inefficient, no interpretability.</td>
	</tr>
</table>

<p>Random forest regression is the same as the random forest classification except for the impurity function used. See <span class="name">decision tree</span> for more information.</p>

<h2>Bootstrap aggregating</h2>

<p>Random forest is an <span class="name">ensemble</span> algorithm, meaning it uses multiple learning algorithms, or one learning algorithm multiple times, to achieve better predictions. Specifically, it applies the <span class="name">bootstrap aggregating (bagging)</span> method to decision trees. Bagging helps reduce variance to avoid overfitting. Given a training set, bagging randomly generates \(n\) subsets of one particular size \(k\), and trains a model on each. It then averages the output, in the case of regression. With random forest, we have \(n\) decision trees built off of random subsets of the data. </p>

<p>Random forest, as opposed to a single decision tree regressor, generates a greater number of nodes. However, since the nodes converge at some point, the quantity is not necessarily proportional to the number of trees. The converged values given a sufficient number of trees can have greater predictive power, and ameliorates some of the concerns related to CART. Specifically, although individual trees might be unstable with respect to small changes, the changes do not affect all trees; therefore, the forest is much more stable. The randomly generated subsets also have a regularizing effect, with helps with overfitting. Despite so, it is still somewhat prone to overfitting, an trait inherent in decision tree algorithms. As one might expect, random forest is still computationally expensive. On the flip side, compared to a single decision tree, random forest loses its easy interpretability.</p>

<p>A typical application might use \(n=500\) trees.</p>

<p>It is recommended to <span class="important">use cross-validation to find the optimal depth to prevent overfitting</span>.</p>