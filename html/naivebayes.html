<table>
	<tr>
		<th class="pros">Pros</th>
		<th class="cons">Cons</th>
	</tr>
	<tr>
		<td class="pros">Efficient, nonlinear, generative, not biased by outliers.</td>
		<td class="cons">Assumes all features have same statistical relevance, since it is biased by a priori.</td>
	</tr>
</table>

<p>Naive Bayes is based on the Bayes Theorem, a cornerstone in the field of probability.</p>

$$ Pr(Y|X) = \frac{Pr(X|Y) Pr(Y)}{Pr(X)}$$

<p>\(Pr(Y|X)\) is called the <span class="name">a posteriori</span> term, Latin for after the fact. It is the probability of the response given that we know the predictors. \(Pr(Y)\) is called the <span class="name">a priori</span> term, Latin for before the fact. It is the probability of the response before we know anything about the predictors. The relation that connects these two is the term \(Pr(X|Y)\), called the <span class="name">likelihood</span>. On the bottom, \(Pr(X)\) is called the <span class="name">marginal likelihood</span>. It is marginal because it frames the problem within the probability of the predictors, instead of the overall probability space. However, the marginal likelihood is a scaling factor irrelevant to \(Y\), the response, so we drop the term for now.</p>

<p>This makes the naive Bayes a <span class="name">generative</span> model, one that learns the joint probability \(Pr(X, Y) = Pr(X|Y) Pr(Y)\) instead of the conditional probability \(Pr(Y|X) = \frac{Pr(X, Y)}{Pr(X)}\). Typically, an algorithm like linear regression learns the response given a number of predictors. These are <span class="name">discriminative</span> models. Naive Bayes learns the probability of predictors given a response. In other words, it looks at how likely it is for each response label to have a set of predictor features. Then, when it comes to predict new data, the compares it against each class. It is, thus, able to generate the data.</p>

<p>Suppose that we drop the marginal likelihood. Then, </p>

$$
\begin{align}
Pr(Y|X) & \propto Pr(X|Y) Pr(Y) \\
& = Pr(x_1, \dots, x_n|Y) Pr(Y) \\
& = Pr(x_1|x_2, \dots, x_n, Y) Pr(x_2|x_3,\dots,x_n,Y)\dots Pr(x_n|Y) Pr(Y)
\end{align}
$$ 

<p>If each feature is binary, notice that \(x_1\) depends on \(n-1\) features, which has \(2^{n-1}\) possibilities. This becomes unwieldly for even a small number of features. This is where the "naive" part comes in. Naive Bayes <span class="important">assumes conditional independence of each feature</span>, that is that each feature is independent from each other given \(Y\). This has the effect of reducing dependence on \(n\) possibilities.</p>

$$
\begin{align}
Pr(Y|X) & \propto Pr(x_1|Y) Pr(x_2|Y)\dots Pr(x_n|Y) Pr(Y) \\
& = Pr(Y) \prod_{i=1}^n Pr(x_i|Y)
\end{align}
$$

<p>In order to avoid underflow and reduce computational complexity, we take the negative log to obtain the <span class="name">negative log likelihood</span>, our cost function.</p>

$$J(X, Y) = Pr(Y) + \sum_{i=1}^n \mathrm{log}(Pr(x_i|Y))$$

<p>The objective is</p>

$$\mathrm{min} \hspace{2mm} J(X, Y)$$

<p>If the probabilities are discrete, then it might be possible to solve for the parameters from a table. Otherwise, there are 3 common methods to estimate the likelihood.
	<ul>
		<li>Gaussian NB. \(p(x|Y) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</li>
		<li>Multinomial NB. \(p(x|Y_k) = \frac{(\sum_i x_i)!}{\prod_i x_i!} \prod_i p_{ki}^{x_i} \)</li>
		<li>Bernoulli NB. \(p(x|Y_k) = \prod_{i=1}^n p_{ki}^{x_i} (1-p_{ki})^{(1-x_i)}\)</li>
	</ul>
where there are \(k\) classes.</p>

<p>Gaussian is typically useful when there is continuous data. Multinomial is when there are multiple classes of data. Bernoulli is for when data is binary. In Python, they are represented by classes <span class="code">sklearn.naive_bayes.GaussianNB</span>, <span class="code">sklearn.naive_bayes.MultinomialNB</span>, and <span class="code">sklearn.naive_bayes.BernoulliNB</span>, respectively.</p>

<h2>Zero Probabilities</h2>

<p>Since all probabilities are multiplied together, it is a problem if there are no samples for a given predictor value (e.g. no samples in which \(x_1 = {FALSE}\)). One possible solution is to add a dummy value if there are no samples, but this could distort the probabilities too much, especially for smaller datasets. Another possible solution is <span class="name">add-one smoothing</span>, also called <span class="name">Laplace smoothing</span>. This method updates each class, such that, instead of</p>

$$Pr(Y|X) = \frac{Pr(X, Y)}{Pr(X)}$$

<p>it becomes</p>

$$Pr(Y|X) = \frac{Pr(X,Y) + 1}{Pr(X) + V}$$

<p>where \(V\) is all the predictor combinations. This ensures that
	<ul>
		<li>Even if \(Pr(X, Y) = 0\), \(Pr(Y|X) \neq 0\).</li>
		<li>All the probabilities still add up to 1.</li>
	</ul>
</p>

<p>This may still give too much weight to unseen classes, so it is more common to use <span class="name">general additive smoothing</span>.</p>

$$
\begin{align}
Pr(Y|X) & = \frac{Pr(X,Y) + \lambda}{Pr(X) + \lambda V} \\
& s.t. \hspace{2mm} 0 \leq \lambda \leq 1
\end{align}
$$

<p>If \(\lambda = 0\), then no smoothing is applied. If \(\lambda = 1\), then it becomes add-one smoothing.</p>