<table>
	<tr>
		<th class="pros">Pros</th>
		<th class="cons">Cons</th>
	</tr>
	<tr>
		<td class="pros">Good scalability with number of samples. Good with small to medium number of clusters, flat geometry.</td>
		<td class="cons">Assumes clusters are convex and isotropic. Responds poorly to irregular shapes or elongated clusters. Susceptible to curse of dimensionality.</td>
	</tr>
</table>

<h2>Clustering</h2>

<p>K-means is a clustering algorithm. Clustering differs from classification in that it is unsupervised. While classification problems begin with a number of labels, and we wish to predict which label future points fall under, clustering problems begin with no labels, and we wish to find out how to separate a dataset into a given number of labels. Often, the number of labels is unknown, so we run the k-means multiple times, while changing \(K\). The algorithm is simple.
	<ol>
		<li>Randomly select \(K\) data points to be the <span class="name">centroids</span> \(\mu_k\), \(j=\left\{1,K\right\}\).</li>
		<li>Iterate through each point \(x^{(i)}\), \(i=\left\{1,m\right\}\), and find the distance between \(x^{(i)}\) and each \(\mu_k\), \(d(x^{(i)}, \mu_k)\).</li>
		<li>Assign \(x^{(i)}\) to the cluster \(C_k\) with the nearest centroid.</li>
		<li>After all points are assigned to a cluster, calculate the means of each cluster, and let that mean be the new centroids.</li>
		<li>Repeat steps 2-4 until there are no more changes.</li>
	</ol>
</p>

<p>The distance algorithm used is usually the Euclidean, hence the name k-mean. However, sometimes other distances are used, like the L1-norm, in which the cluster centers are <span class="name">medoids</span>, and the algorithm becomes k-medoids. For the rest of this discussion, we assume the standard. </p>

<h2>K-Means++</h2>

<p>Since the initial centroids are randomly selected, there are cases where the resulting clusters are clearly sub-optimal. For example, suppose there are 3 populations centered around the corners of an isoceles triangle, with 2 populations close to each other. An optimal cluster would correctly label these clusters. However, if the initial seeds put 2 centroids in the furthest population, and 1 within the 2 closer populations, then the 2 closer populations would form 1 cluster, and the furthest population would be split into 2 clusters. A recent improvement is the <span class="name">k-means++</span> algorithm, which changes the method of the initial seeding process.
	<ol>
		<li>Choose one centroid at random from the data points.</li>
		<li>Iterate through each point \(x^{(i)}\), and computer the distance \(d(x^{(i)}, \mu_k)\) to the nearest centroid.</li>
		<li>Choose a new centroid at random from the data points, but using a weighted probability distribution where \(x^{(i)}\) is chosen with probability proportional to \(d^2\).</li>
		<li>Repeat steps 2-3 until \(K\) centroids have been chosen.</li>
	</ol>
</p>

<p>A method prior to k-means++ might have run k-means multiple times to find the best results. The authors of k-means++ claims that, despite the extra steps for initial seeding, k-means++ typical improves speed by 2 times, since it converges faster, and improves accuracy up to 1000 times (<a href="https://web.archive.org/web/20060209181757/http://www.cs.umd.edu/~mount/Papers/kmlocal.pdf">source</a>). This method is the default of parameter <span class="code">init='k-means++'</span> in Python. However, in R, it requires the <span class="code">ClusterR</span> package.</p>

<h2>Maximizing Distance Between Clusters</h2>

<p>Mathematically speaking, the objective of k-means is to minimize the <span class="name">within cluster sum of squares (WCSS)</span>, also the <span class="name">inertia</span>. This value specifies the distance between each point to the centroid of a cluster. The inertia of a cluster \(C_k\) is</p>

$$
\begin{align}
I(C_k) & = \sum_{x^{(i)} \in C_k} d^2(x^{(i)}, \mu_k) \\
& = \sum{x_i \in C_k} ||x_i - \mu_k||^2
\end{align}
$$

<p>Note that \(\mu_k\) are the centroids of each cluster \(C_k\), defined as</p>

$$\mu_k = \frac{1}{m} \sum_{x_i \in C_k} x^{(i)}$$

<p>assuming each point is weighted equally. The sum of WCSS of all \(K\) clusters is</p>

$$WCSS_{TOT} = \sum_{k=1}^K I(C_k)$$

<p><span class="name">Huygens Theorem</span> states that minimizing \(WCSS_{TOT}\) is equivalent to maximizing the <span class="name">between cluster sum of squares (BCSS)</span>. In other words, if we want to maximize the distance between each cluster, then we want to minimize the distance within each cluster. Informally, this is because</p>

$$SS_{TOT} = WCSS + BCSS$$

<h2>Picking the Best \(K\)</h2>

<p>There are a number of heuristics for picking the optimal \(K\), but ultimately, many cases may come down to visual inspection via the <span class="name">elbow method</span>. This is one of the major weaknesses of k-means.</p>