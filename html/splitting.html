<p>This code creates \(k\) randomized folds in a feature matrix, in order to be plugged into an algorithm of choice. It performs k-folds cross-validation for the purposes of testing how well the model generalizes.</p>

<p>Machine learning estimators typically learn a hypothesis on a <span class="name">training set</span>, then applies the hypothesis in attempt to predict values that are not in the set. Since we wish to test that out-of-sample prediction, we split our data into the training and <span class="name">test set</span> so we can make sure that our prediction generalizes, a process called <span class="name">cross-validation</span>. Ideally, we would want as much of the data as possible to go into training the hypothesis, since this improves accuracy and reduces the chance of overfitting. However, if the test set is smaller, then it will have greater variance. Therefore, we need to pick an appropriate split between training and test set.
</p>
<p>For smaller datasets, splitting the dataset into training and test sets of 	80/20 or 70/30, respectively, is typical. For larger datasets, we may want to use a smaller training set to reduce computational complexity; or, in fact, more could go into the training set to minimize variance in the hypothesis. For example, 80/20 split is appropriate for datasets with 1,000 examples, but 95/5 might suffice for datasets with 100,000 examples.
</p>

<p>K-folds cross-validation splits the dataset into \(k\) subsets, where \(k-1\) subsets become the training set, and the remaining subset becomes the test set. The process is repeated \(k\) times, and some kind of metric is used to average or combine the estimation results. This allows us to use all of the data towards training, which is especially pertinent for smaller datasets.</p>

<p>For this method, Python has the <span class="code">sklearn.model_selection.KFold</span> class.</p>

<pre><code class="language-python">
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold
kf = KFold(n_splits=10, shuffle=False)
for train_index, validation_index in kf.split(X):
	X_train, X_val = X[train_index], X[validation_index]
	y_train, y_val = y[train_index], y[validation_index]
	
	# Fit an example model to training set
	model = LinearRegression()
	model.fit(X_train, y_train)

	# Get score of model
	print(model.score())
</code></pre>

<p>But a simpler method is to use <span class="code">sklearn.model_selection.cross_val_score</span> function to automate the process of creating \(k\) folds, and scoring the model, returning a \(k\)-vector array. The array could then be averaged to produce a meaningful score. The scoring function used depends on the model function itself. Regression models like <span class="code">sklearn.linear_model.LinearRegression</span> might use \(R^2\). Classification models like <span class="code">sklearn.svm.SVC</span> might use mean accuracy. Typically, after scoring a number of models, the best one is chosen to be trained on the entire dataset as the final model.</p>

<p>Of course, <span class="code">cross_val_score()</span> takes in as an argument the model used, in order to train and use its scoring function. So most models in the Python stack are compatible with this feature and provides the <span class="code">score()</span> function. Similarly, most models in the R stack accepts the <span class="code">trainControl</span> object of the <span class="code">caret</span> library for their <span class="code">trControl</span> parameter. If the <span class="code">method</span> parameter of <span class="code">trainControl</span> is passed <span class="code">"cv"</span>, then it instructs for the model to be cross-validated.